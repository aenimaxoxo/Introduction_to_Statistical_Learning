---
title: "ISL_Ch10"
author: "Michael Rose"
date: "October 29, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
```

# Principal Components Analysis

We will be performing principal components analysis on the USArrests data set. The USArrests data set contains arrest data from 50 states, with variables for Murder, Assault, Urban Population, and Rape. 

```{r, echo=TRUE}
# Get row names
states <- row.names(USArrests)
states
# Find variable names
names(USArrests)
# Find means for variables. Apply() allows us to apply the mean() function to each row or column of the data set
apply(USArrests, 2, mean)
```

We see from above, that on average there are 3 times as many rapes as murders and 8.5 times as many assaults as rapes. Lets view the variance: 

```{r, echo=TRUE}
apply(USArrests, 2, var)
```
From above we see that each of the variables has vastly different variances. 

The UrbanPop variable measures the percentage of the population in each state living in an urban area, which is not a comparable number to the number of rapes in each state per 100,000 individuals. 

We must standardize the variables, otherwise the assault variable will dominate the PCA due to its large mean and variance. We will standardize to mean 0 and a standard deviation of one. 

```{r, echo=TRUE}
# prcomp performs principal component analysis. By default the prcomp function centers the variables to have mean 0 and scale=TRUE scales the variables to have a standard deviation of 1. 
pr.out <- prcomp(USArrests, scale=TRUE)

# see components of prcomp
names(pr.out)
```

The center and scale components correspond to the means and standard deviations of the variables that were used for scaling prior to implementing PCA. The rotation matrix provides the principal component loadings; each column of pr.out$rotation contains the corresponding principal component loading vector. 

```{r, echo=TRUE}
# mean values pre scale
pr.out$center

# Standard Deviation values pre scale
pr.out$scale

# Principal Component Loadings
pr.out$rotation
```

From above we see that there are 4 distinct principal components, which is equivalent to our p variables. 

The prcomp() function has an x parameter that contains the principal component score vectors. 

```{r, echo=TRUE}
# dimensions of pca matrix
dim(pr.out$x)

# plot a biplot with first 2 principal components
#biplot(pr.out, scale=0)

# make it look less ugly
pr.out$rotation <- -pr.out$rotation
pr.out$x <- -pr.out$x
biplot(pr.out, scale=0)

# still really ugly

# look at new scaled standard deviations
pr.out$sdev

# look at new scaled variance
pr.var <- pr.out$sdev^2
pr.var

# compute proportion of variance explained by each principal component
pve <- pr.var / sum(pr.var)
pve
```

We can see from above that the first principal component accounts for 62% of the total variance, the 2nd accounts for 24.7%, and so on. We can plot the PVE and cumulative PVE:

```{r, echo=TRUE}
# plot pve
plot(pve, xlab="Principal Component", ylab="Proportion of Variance Explained", ylim=c(0,1), type='b')

# plot cumulative pve
plot(cumsum(pve), xlab="Principal Component", ylab = "Cumulative Proportion of Variance Explained", ylim=c(0, 1), type='b')
```


# Clustering

## K-Means Clustering

```{r, echo=TRUE}
# reproducibility
set.seed(8)

# create matrix of simulated values
x <- matrix(rnorm(50*2), ncol=2)
x[1:25, 1] <- x[1:25, 1]+3
x[1:25, 2] <- x[1:25, 2]-4

# Perform K-Means clustering with K = 2
km.out <- kmeans(x, 2, nstart=20)

# see cluster assignment for each observation
km.out$cluster

# plot the data with each assignment color coded
plot(x, col=(km.out$cluster+1), main="K-Means Clustering Results with K=2", xlab="", ylab="", pch=20, cex=2)

```

In the example above, we knew there was 2 clusters because we generated the data. For real data, we don't know the true number of clusters. Lets look at the same data plotted with K=3:

```{r, echo=TRUE}
# reproducibility
set.seed(16)
# perform K-means. The nstart parameter is the number of initial cluster assignments. If nstart > 1 is used, then K-Means will be performed using multiple random assignments, and k-means will report only the best results
km.out <- kmeans(x, 3, nstart=20)
km.out

# compare using nstart = 1 to nstart = 20
# reproducibility
set.seed(24)
# kmeans with 1 initial cluster assignment
km.out <- kmeans(x, 3, nstart=1)
# total within cluster sum of squares
km.out$tot.withinss
# kmeans with 20 initial cluster assignments
km.out <- kmeans(x, 3, nstart=20)
# total within cluster sum of squares
km.out$tot.withinss
```

It is strongly recommended to run k-means clustering with large values of nstart, since otherwise we may reach an undesirable local optimum. When performing k-means, it is aso important to set a random seed so that the initial cluster assignments in step 1 of the algorithm can be replicated and the k-means output will be fully reproducible. 

## Hierarchical Clustering

In R the hclust() function implements hierarchical clustering. We will be using the USArrests data to plot the hierarchical dendrogram using complete, single, and average linkage clustering with euclidean distance as the dissimilarity measure. 

```{r, echo=TRUE}
# hierarchical clustering using complete linkage and euclidean distance matrix
hc.complete <- hclust(dist(x), method="complete")
# average linkage
hc.average <- hclust(dist(x), method="average")
# single linkage
hc.single <- hclust(dist(x), method="single")

# plot 
par(mfrow=c(1,3))
# complete
plot(hc.complete, main="Complete Linkage", xlab="", sub="", cex=0.9)
# average
plot(hc.average, main="Average Linkage", xlab="", sub="", cex=0.9)
# single
plot(hc.single, main="Single Linkage", xlab="", sub="", cex=0.9)

```

To determine the cluster labels, we can use the cutree() function:

```{r, echo=TRUE}
cutree(hc.complete, 2)
cutree(hc.average, 2)
cutree(hc.single, 2)
```

To scale the variables before performing hierarchical clustering of the observations, we use the scale() function:

```{r, echo=TRUE}
xsc <- scale(x)
plot(hclust(dist(xsc), method="complete"), main="Hierarchical Clustering with Scaled Features")
```

Correlation based distancce can be computed using the as.dist() function, which converts an arbitrary square symmetric matrix into a form that the hclust() function recognizes as a distance matrix. This only makes sense for data with at least 3 features since the absolute correlation between any 2 observations with measurements on 2 features is always 1.

```{r, echo=TRUE}
x <- matrix(rnorm(30*3), ncol=3)
dd <- as.dist(1-cor(t(x)))
plot(hclust(dd, method="complete"), main="Complete Linkage with Correlation Based Distance", xlab="", sub="")
```

# NC160 Data Example

The following data is a cancer cell line microarray, consisting of 6830 gene expression measurements on 64 cancer cell lines. 

```{r, echo=TRUE}
nci.labs <- NCI60$labs
nci.data <- NCI60$data
dim(nci.data)
```

Each cell line is labelled with a cancer type. There are 64 rows and 6830 columns. We begin by examining the cancer types for the cell lines: 

```{r, echo=TRUE}
nci.labs[1:4]
table(nci.labs)
```

We first perform PCA on the data after scaling the variables (genes) to have standard deviation of one

```{r, echo=TRUE}
pr.out <- prcomp(nci.data, scale=TRUE)
```

We will not plot the first few principal component score vectors to visualize the data. The observations (cell lines) corresponding to a given cancer type will be plotted in the same color. First we must create a simple function that assigns a distinct color to each of the 64 cell lines, based on the cancer type to which it corresponds. 

```{r, echo=TRUE}
# rainbow() takes a positive integer and returns a vector containing that number of distinct colors
Cols <- function(vec){
  cols <- rainbow(length(unique(vec)))
  return(cols[as.numeric(as.factor(vec))])
}
```

```{r, echo=TRUE}
par(mfrow=c(1,2))
plot(pr.out$x[,1:2], col=Cols(nci.labs), pch=19, xlab="Z1", ylab="Z2")
plot(pr.out$x[,c(1,3)], col=Cols(nci.labs), pch=19, xlab="Z1", ylab="Z3")
```

From the plots above, we can see a flurry of sprinkles. We do also see some clustering for cancers of a single type (each color). This indicates that cell lines from the same cancer type tend to have pretty similar gene expression levels. 

```{r, echo=TRUE}
summary(pr.out)
plot(pr.out)
```

The summary above shows the proportion of variance explained for the first few principal components, and the plot shows how the proportion drops off over time. The bar plot isn't as useful visually as a scree plot: 

```{r, echo=TRUE}
pve <- 100*pr.out$sdev^2/sum(pr.out$sdev^2)
par(mfrow=c(1,2))
plot(pve, type="o", ylab="PVE", xlab="Principal Component", col="blue")
plot(cumsum(pve), type="o", ylab="Cumulative PVE", xlab="Principal Component", col="brown3")
```

From the plots above, we see that the first 7 principal components explain around 40% of the variance in the data. We can see from the scree plot on the left that after the 7th principal component there is a sharp drop off with respect to the amount of variance reduced by adding more principal components. 

## Clustering the Pbservations of the NCI60 data 

We will now proceed to hierarchically cluster the cell lines in the NCI60 data, with the goal of finding out whether or not the observations cluster into distinct types of cancer. 

```{r, echo=TRUE}
# scale the data
sd.data <- scale(nci.data)
# plot complete, single and average linkage
par(mfrow=c(1, 3))

# create distribution with scaled data
data.dist <- dist(sd.data)

# Complete
plot(hclust(data.dist), labels=nci.labs, main="Complete Linkage", xlab="", sub="", ylab="")

# Average
plot(hclust(data.dist, method="average"), labels=nci.labs, main="Average Linkage", xlab="", sub="", ylab="")

# Single
plot(hclust(data.dist, method="single"), labels=nci.labs, main="Single Linkage", xlab="", sub="", ylab="")
```

We can see from above that the choice of linkage strongly affects the results obtained. We can cut the dendrogram at the height that will yeild a particular number of clusters. Here we choose 4: 

```{r, echo=TRUE}
hc.out <- hclust(dist(sd.data))
hc.clusters <- cutree(hc.out, 4)
table(hc.clusters, nci.labs)
```

From the table above we can see that there are some patterns. For example, all the leukemia observations are in cluster 3. 

```{r, echo=TRUE}
par(mfrow=c(1,1))
plot(hc.out, labels=nci.labs)
abline(h=139, col="red")
```

Now that we have a dendrogram that is split into 4 groups, we can compare the results with those obtained from kmeans clustering (K = 4)

```{r, echo=TRUE}
# reproducibility
set.seed(32)
km.out <- kmeans(sd.data, 4, nstart=20)
km.clusters <- km.out$cluster
table(km.clusters, hc.clusters)
```

We see from the table above that the clusters are somewhat different. Cluster 2 in K means is identical to cluster 3 in hierarchical clustering. However, the other clusters differ - ex: in cluster 4 k-means contains a portion of the observations assigned to cluster 1 by hierarchical clustering, as well all of the observations assigned to cluster 2 by hierarchical clustering. 